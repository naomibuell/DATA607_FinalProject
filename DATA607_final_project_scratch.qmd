---
title: "Final Project"
author: "Naomi Buell, Nick Kunze, and Kaylie Evans"
date: Sys.Date()
format: html
editor: visual
---

```{r}
#| label: load packages
#| message: false
library(tidyverse)
library(janitor)
library(jsonlite)
library(qdapDictionaries)
library(tidytext)
library(rvest)
library(vader)
```

## Introduction

We signed up for the [Article Search API](https://developer.nytimes.com/docs/articlesearch-product/1/overview) from the New York Times. Below is an interface in R to read in the JSON data and transform it into an R data frame. We also merge this data with data from Letterboxd.

## Connect to NYT API

First, we sign up for an API key on the New York Times website. Here, we use the `rstudioapi::skForPassword()` function to keep my API key private when running the code in R Studio. For running and rendering all code for the purposes of this markdown, we also alternatively save the API key in an R chunk that we elect not to include in this published version.

The base HTTP request URL is defined below as well.

```{r}
#| label: set up API
#| error: true
#| message: false
#| warning: false
#| results: hide
api_key <- rstudioapi::askForPassword("Authorization Key")

base_url <-
  "https://api.nytimes.com/svc/search/v2/articlesearch.json?"
```

```{r}
#| include: false
# Alternative to API key generated with the askForPassword function for the purposes of rendering my QMD
api_key <- "mC1y5Hr361gaqmvkjHpGd6WUdiL917vA"
```

Here, we create is a function to pull data based on the filter and page number parameters, starting from the newest articles.

Note that the Article Search API returns a max of 10 results at a time. We use the page query parameter to paginate through results (page=0 for results 1-10, page=1 for 11-20, etc. You can paginate through up to 100 pages (1,000 results)). Also note that we add a delay in the loop to avoid hitting the rate limit and getting a 429 error.

```{r}
#| label: create function
get_movies <- function(filter, num_pages, timeout) {
  # initialize data frame
  df <- tibble()
  
  for (page in seq_len(num_pages)) {
    # set url
    url <- paste0(base_url,
                  "fq=",
                  filter,
                  "sort=newest&page=",
                  page,
                  "&api-key=",
                  api_key)
    
    # initialize success as false before we get a success
    success <- FALSE
    
    while (success == FALSE) {
      # while success is false,
      tryCatch({
        df <- fromJSON(url, flatten = TRUE)$response$docs |>
          clean_names() |> rbind(df, .data) # append to df
        success <- TRUE # Set success to TRUE if no error occurs
      },
      
      # if error,
      error = function(e) {
        # Add a delay between requests to avoid hitting the rate limit
        Sys.sleep({timeout})
      })
    }
  }
  
  # Return the resulting data frame
  return(df)
}
```

## Load JSON NYT data into R data frame

We call the function, iterating through `num_pages` pages of JSON data, appending them together as the data frame `movies_df`. We will be using the Article Search API to get New York Times movie reviews, so we define the filter query accordingly.

```{r}
#| label: load data from API
#| message: false
#| warning: false

filter <-
  'section_name%3A%22Movies%22%20AND%20type_of_material%3A%22Review%22'
num_pages <- 100
num_seconds <- 9

nyt_movies_raw <- get_movies(filter, num_pages, num_seconds)
```

Here is the R data frame of NYT JSON movie review data loaded from the NYT API:

```{r}
#| label: browse df

head(nyt_movies_raw)
```

## Cleaning NYT data

Before we finalize this data set, we subset the data frame to our variables of interest and do some data transformation operations:

-   The `keywords` variable in the original data frame was a list-column. The `name` of the movie is tagged as a keyword in each review article. We unnest this list-column variable to extract the movie `name`.

-   Clean the movie `name` by

    -   converting all characters to lowercase

    -   removing parentheticals in the string for matching using REGEX

    -   removing any ", the" at the end of any movie names in the NYT data

-   Convert NYT publication date column `pub_date` to a datetime format.

```{r}
#| label: clean variables

# Define REGEX patterns
media_pattern <-
  ".*\\(([^\\(\\)]+)\\)$" # to get media type (e.g., "movie", "play", etc.), pull text in final parenthetical of movie title
name_pattern <-
  "(.*) \\(([^\\(\\)]+)\\)$" # to get movie title on it's own, remove final parenthetical
the_pattern <- "^the\\s" # to remove "the" from beginning of movie title
comma_the_pattern <- ",\\sthe$" # to remove ", the" from end of movie title
a_pattern <- "^a\\s" # to remove "a" from beginning of movie title
comma_a_pattern <- ",\\sa$" # to remove ", a" from end of movie title

nyt_movies <-  nyt_movies_raw |>
  # Unnest `keywords` list variable
  unnest(keywords, keep_empty = TRUE) |>
  # Correct date column format
  mutate(
    pub_date = as_datetime(pub_date),
    media = str_match(value, media_pattern)[, 2] |>
      tolower(),
    # Clean movie titles, removing "the"
    name = str_match(value, name_pattern)[, 2] |>
      tolower() |>
      str_replace(comma_the_pattern, "") |>
      str_replace(the_pattern, "") |>
      str_replace(a_pattern, "") |>
      str_replace(comma_a_pattern, "")
  ) |>
  # Filter data to movies only (removing keyword rows for crew, other types of media, etc.)
  filter(media == "movie") |>
  # Reorder reviews by publication date
  arrange(desc(pub_date)) |>
  # Select variables of interest: columns w/ text for sentiment analysis and merging
  select(
    abstract,
    web_url,
    name,
    pub_date,
    headline_main,
    headline_kicker,
    headline_print_headline
  )

head(nyt_movies)
```

Above is our merged data set.

## Load Letterboxd data and clean

Load Letterboxd data and clean variables. We perform the same cleaning transformations to the movie `name` variable that we did with the variable of the same name in the NYT data above. Also, we drop movies released before the earliest review in the NYT data (`r min(nyt_movies$pub_date)`); we don't want to merge these older Letterboxd movies since it's unlikely that an NYT critic would review a movie more than a year after its release.

```{r}
#| label: load letterboxd
  
# load letterboxd data
letterboxd_movies <-
  read_csv(
    "https://raw.githubusercontent.com/naomibuell/DATA607_FinalProject/main/movies_trimmed.csv"
  ) |>
  drop_na(minute) |>
  mutate(
    name = tolower(name) |> # switching movie names to lower case
      # Use str_match() with the pattern
      str_replace(comma_the_pattern, "") |>
      str_replace(the_pattern, "") |>
      str_replace(a_pattern, "") |>
      str_replace(comma_a_pattern, "")
  ) |>
  select(-c(id)) |> 
  # only keep Letterboxd data within a similar date range to the NYT data
  filter(date >= year(min(nyt_movies$pub_date)) - 1)

head(letterboxd_movies)
```

## Merge NYT and Letterboxd Data

Below, we merge this NYT data with Letterboxd data based on movie `name`. We also pick the best matches based on when the NYT critic reviewed the movie and when Letterboxd says the movie was released, assuming a true match would show the movie reviewed by NYT right when it came out (since Letterboxd only posts the year of release and not the full date, we assume all movies came out on Christmas day for the purposes of this calculation).

```{r}
#| label: merge Letterboxd and NYT
#| warning: false
#| message: false

# First, merge datasets by name. This is a many to many join, so movie-review rows will not be unique, with some extra matches done in error.
merged <- inner_join(nyt_movies, letterboxd_movies) |>
  
  # For movies w/ multiple matched rows, choose absolute difference in dates between sources.
  group_by(name) |>
  mutate(
    # assuming all movies came out on Christmas (popular date for movie release),
    release_date = as_datetime(paste0(date, "-12-25")),
    # calculate absolute diff btwn review and release date
    dates_diff = abs(difftime(pub_date, release_date, units = "days"))
  ) |> 
  filter(dates_diff == min(dates_diff)) |> # for each unique name, get movie row w/ shortest diff between review and release date
  ungroup() |>
  # remove any duplicated rows
  distinct()

head(merged)
```

## Describe and Validate Analysis Data set

Now that we have our analysis data set, we can perform a few statistical analyses to describe and validate the merged data. The following section checks our data set that we'll use for analysis for outliers, consistency, completeness, and any discrepancies between the merged data sets that would indicate merge errors.

### Descriptive statistics

First, we generate summary statistics and compare the distribution of the dates reported by both data sets:

```{r}
#| label: numeric descriptive stats - dates
#| message: false
#| warning: false

merged  |> 
  select(pub_date, date) |> 
  summary()

# compare dates for NYT and Letterboxd data
merged |>
  ggplot() +
  geom_histogram(
    aes(x = pub_date, fill = "Merged Review publication date (source: NYT)"),
    alpha = .5,
    bins =  44
  ) +
  geom_histogram(
    aes(x = release_date, fill = "Merged Movie release year (source: Letterbox)"),
    alpha = .5,
    bins = 44
  ) +
  theme(legend.position = "bottom") +
  labs(title = "Dates, by data source",
       x = "Date",
       y = "Frequency")
```

Both movie release and review publication dates align well between data sources. They start and end around the same time. Both have peaks in the early 2000's and around 2020's, with a major dip in the late 2000's. This lack of data around the late 2000's stems from the limited NYT data, not the Letterboxd data.

Next we'll check the distribution and outliers of the other numerical data, all from the Letterboxd data set:

```{r}
#| label: Check movie duration (mins) for outliers

summary(merged$minute)

merged |>
  ggplot(aes(x = minute/60)) +
  geom_histogram(bins = 60) +
  labs(title = "Movie length",
       x = "Length in hours (source: Letterboxd)")
```

The distribution is centered around about `r round(median(merged$minute/60), 2)` hours. There are many high outliers in terms of movie length (max `r round(max(merged$minute/60))` hours)–likely due to full television or movie series being included in the database. We elect not to remove these outliers because they can be genuine matches. For e.g., the longest film in the data, "Heimat" was 924 minutes long according to Letterboxd and was reviewed by the NYT, confirming that this chronicle of Germany "will be broadcast for the first time on cable on Bravo in eight parts."

Next we check user ratings for outliers:

```{r}
#| label: summarize rating
#| message: false
#| warning: false

summary(merged$rating)

merged |>
  ggplot(aes(x = rating)) +
  geom_histogram() +
  labs(title = "User ratings",
       x = "Average movie user rating (source: Letterboxd)")
```

Average movie ratings in this data-set range from `r min(merged$rating)` to `r max(merged$rating)`. The average average `rating` on Letterboxd is `r mean(movies$rating)`. The data looks normal but is slightly left skewed. This is expected.

```{r}
#| label: Check ratings for outliers

merged |>
  ggplot(aes(x = rating)) +
  geom_boxplot() 
```

### **Data Quality Assessment/Overlap Analysis**

Here, we check the number of records and variables from each source that were merged. The Letterboxd data was much larger than what we pulled from NYT using the API (`r nrow(letterboxd_movies)` vs. `r nrow(nyt_movies_raw)` movies long, respectively), so the NYT data was the limiting factor for the merge in terms of the length of our final data set.

```{r}
#| label: percent merged

perc_merged <- nrow(merged)/nrow(nyt_movies)
```

However, `r round(perc_merged)*100` percent of data from NYT was able to be merged with a Letterboxd rating.

Here, we identify any duplicate records:

```{r}
#| label: check duplicate movie rows

duplicates <- merged |>
  group_by(name) |>
  mutate(duplicate = n() > 1) |>
  filter(duplicate)

head(duplicates)

num_duplicated_rows <- duplicates |>
  nrow()/2
```

There are `r duplicate_rows` duplicated movies, where one movie `name` from the NYT matched to two observations with the same movie `name` (and release year) in the Letterboxd database. We can manually review to determine which observation is a true match, then drop the other observation(s). A good way to automate the checking of true matches, which may be necessary with larger datasets, could be to group by multiple columns along with *name* such as *pub_date* or *minute* (movie length). In this case, each of these are true matches, and therefore all duplicates can be removed safely.

```{r}
#| label: remove duplicates

merged_no_dupl <- merged |> 
  distinct(name, .keep_all = TRUE)

head(merged_no_dupl)
```

## Web Scraping Full NYT Reviews

As the API only provides the first paragraph/snippet from the movie review, we'll need to scrape the HTML hosted on the NYT website for the full text. Luckily, the API provides the URI via the web_url field and every element containing review text contains the unique class StoryBodyCompanionColumn. This allows us to easily pull the text from the HTML elements as a character vector and collapse it into a single string, adding it as a new column in our data frame.

We do all of this after the merging and cleaning to minimize the amount of scraping we need to perform to only the movies reviews we will be analyzing, reducing our HTML requests by almost half and memory utilization by over 2 megabytes. That may seem small, but for larger datasets, 50% reduction is a large amount.

We have a NYT digital subscription, preventing any license agreement issues concerning the text.

```{r}
reviewScrape <- function(url) {
  success <- FALSE
  
  while (success == FALSE) {
    tryCatch({
      htmlText <- read_html(url) |>
        html_nodes(".StoryBodyCompanionColumn") |>
        html_text() |>
        paste0()
      success <- TRUE # Set success to TRUE if no error occurs
    }, error = function(e) {
      print(paste0("Error: ", e))
      Sys.sleep(num_seconds)
    })
  }
  
  # return review text
  return(htmlText)
}
```

```{r}
reviews <- lapply(merged_no_dupl$web_url, reviewScrape)

merged_no_dupl$review <- reviews
```

## Sentiment and Regression Analysis of NYT Review

Now that we have our merged and tidied data, we can perform some deeper analysis of these reviews provided by the NYT API. When performing sentiment analysis, choosing the right dictionary, or lexicon, is important. There already exist [analyses of](https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-017-0121-9) [different lexicons](https://medium.com/@laurenflynn1211/comparing-sentiment-analysis-dictionaries-in-r-c695fca64326), but we will replicate some of those thoughts and analyses here. In general, it's important to choose domain-specific lexicons to ensure proper sentiment analysis of words used and highest possible coverage of words in the text being analyzed.

The three lexicons we'll be utilizing against our movie reviews are AFINN, as it is a lexicon from the tidytext package we've used previously, VADER, a lexicon specifically attuned to sentiments expressed in social media, and labMT, as its dictionary was partially built directly from New York Times publications; they all use a numerical scale for rating the *positivity*, *valence,* or *happiness* of a word.

labMT and AFINN require us to load up our dictionaries to join with our tokenized reviews, but VADER provides a usefull get_vader() function that returns the score of each word in the text as well as the average.

```{r}
#| label: loading of sentiment dictionaries

data(labMT)
afinn <- get_sentiments("afinn")
```

```{r}
#| label: movies sentiment analysis function

sentimentAnalysis = function(movie, output){
  # Break up our reviews into word vector, minus stop words
  reviewWords <- as.tibble(movie["review"]) %>% 
    unnest_tokens(word,value) %>% 
    anti_join(stop_words)
  
  movie["afinn"] <- reviewWords %>% 
    inner_join(afinn) %>%
    summarise(sentiment = mean(value)) %>%
    mutate(movie = fg$name)
  movie["labmt"] <- reviewWords %>% 
    inner_join(labMT) %>%
    summarise(sentiment = mean(happiness_average)) %>%
    mutate(movie = fg$name)
  movie["vader"] <- get_vader(movie["review"][[1]])[2]
  return(movie)
}
```

```{r warning=FALSE}
# Suppressed to hide join by messages
sa <- suppressMessages(do.call(rbind.data.frame, apply(merged_no_dupl, 1, sentimentAnalysis)))
```

```{r}
par(mfrow = c(1, 3))

cor(as.numeric(sa$rating),sa$afinn)
cor(as.numeric(sa$rating),sa$labmt)
cor(as.numeric(sa$rating),as.numeric(sa$vader))

plot(sa$afinn,sa$rating, main="AFINN",
  xlab="NYT Sentiment Mean", ylab="Letterboxd Rating")
plot(sa$labmt,sa$rating, main="LabMT",
  xlab="NYT Sentiment Mean", ylab="Letterboxd Rating")
plot(sa$vader,sa$rating, main="VADER",
  xlab="NYT Sentiment Mean", ylab="Letterboxd Rating")
```

I would expect a somewhat linear relationship between the sentiment analyses and the critic movie rating by letterboxd, but it appears there is none. With these three lexicons, I would be better of with vader, but not by much. For such a niche medium as "movie reviews", it would likely be necessary to use a specialized lexicon built around very specific terms used by movie critics, or even better to use a real ML model to infer letterboxd rating based on sentiment.

Create graphic:

```{r}
#| label: conclusion graphic
```

## Conclusion

\[Add conclusion text here\]

## **References**

Dodds, Peter Sheridan, Kameron Decker Harris, Isabel M. Kloumann, Catherine A. Bliss, and Christopher M. Danforth. “Temporal patterns of happiness and information in a global social network: Hedonometrics and Twitter.” *PLoS ONE* 6, no. 12 (2011).
