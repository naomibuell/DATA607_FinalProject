---
title: "Final Project"
author: "Naomi Buell, Nick Kunze, and Kaylie Evans"
date: Sys.Date()
format: html
editor: visual
---

```{r}
#| label: load packages
#| message: false
library(tidyverse)
library(janitor)
library(jsonlite)
library(tibble)
library(foreach)
library(doParallel)
```

## Introduction

We signed up for the [Article Search API](https://developer.nytimes.com/docs/articlesearch-product/1/overview) from the New York Times. Below is an interface in R to read in the JSON data and transform it into an R data frame. We also merge this data with data from Letterboxd.

## Connect to NYT API

First, we sign up for an API key on the New York Times website. Here, we use the `rstudioapi::skForPassword()` function to keep my API key private when running the code in R Studio. For running and rendering all code for the purposes of this markdown, we also alternatively save the API key in an R chunk that we elect not to include in this published version.

The base HTTP request URL is defined below as well.

```{r}
#| label: set up API
#| error: true
#| message: false
#| warning: false
#| results: hide
api_key <- rstudioapi::askForPassword("Authorization Key")

base_url <-
  "https://api.nytimes.com/svc/search/v2/articlesearch.json?"
```

```{r}
#| include: false
# Alternative to API key generated with the askForPassword function for the purposes of rendering my QMD
api_key <- "mC1y5Hr361gaqmvkjHpGd6WUdiL917vA"
```

Here, we create is a function to pull data based on the filter and page number parameters, starting from the newest articles.

Note that the Article Search API returns a max of 10 results at a time. We use the page query parameter to paginate through results (page=0 for results 1-10, page=1 for 11-20, etc. You can paginate through up to 100 pages (1,000 results)). Also note that we add a delay in the loop to avoid hitting the rate limit and getting a 429 error.

```{r}
#| label: create function
get_movies <- function(filter, num_pages, timeout) {
  # Initialize data frame
  df <- tibble()
  
  # Initialize parallel processing
  registerDoParallel()
  
  # Use foreach with %dopar% to parallelize the loop
  df <- foreach(page = seq_len(num_pages), .combine = rbind) %dopar% {
    # set url
    url <- paste0(base_url,
                  "fq=",
                  filter,
                  "sort=newest&page=",
                  page,
                  "&api-key=",
                  api_key)
    
    # initialize success as false before we get a success
    success <- FALSE
    
    # Fetch data for the current page
    while (!success) {
      tryCatch({
        page_df <- fromJSON(url, flatten = TRUE)$response$docs |>
          as_tibble() |>
          clean_names() |>
          mutate(page = page)  # Add page number as a column
        success <- TRUE
      },
      error = function(e) {
        # Add a delay between requests to avoid hitting the rate limit
        Sys.sleep(timeout)
      })
    }
    
    # Return the data for the current page
    return(page_df)
  }
  
  # End parallel processing
  stopImplicitCluster()
  
  # Return the resulting data frame
  return(df)
}

```

## Load JSON NYT data into R data frame

We call the function, iterating through `num_pages` pages of JSON data, appending them together as the data frame `movies_df`. We will be using the Article Search API to get New York Times movie reviews, so we define the filter query accordingly.

```{r}
#| label: load data from API
#| message: false
#| warning: false

filter <-
  'section_name%3A%22Movies%22%20AND%20type_of_material%3A%22Review%22'
num_pages <- 100
num_seconds <- 9

nyt_movies_raw <- get_movies(filter, num_pages, num_seconds)
```

Here is the R data frame of NYT JSON movie review data loaded from the NYT API:

```{r}
#| label: browse df

head(nyt_movies_raw)
```

## Cleaning NYT data

Before we finalize this data set, we subset the data frame to our variables of interest and do some data transformation operations:

-   The `keywords` variable in the original data frame was a list-column. The `name` of the movie is tagged as a keyword in each review article. We unnest this list-column variable to extract the movie `name`.

-   Clean the movie `name` by

    -   converting all characters to lowercase

    -   removing parentheticals in the string for matching using REGEX

    -   removing any ", the" at the end of any movie names in the NYT data

-   Convert NYT publication date column `pub_date` to a datetime format.

```{r}
#| label: clean variables

# Define REGEX patterns
media_pattern <-
  ".*\\(([^\\(\\)]+)\\)$" # to get media type (e.g., "movie", "play", etc.), pull text in final parenthetical of movie title
name_pattern <-
  "(.*) \\(([^\\(\\)]+)\\)$" # to get movie title on it's own, remove final parenthetical
the_pattern <- "^the\\s" # to remove "the" from beginning of movie title
comma_the_pattern <- ",\\sthe$" # to remove ", the" from end of movie title
a_pattern <- "^a\\s" # to remove "a" from beginning of movie title
comma_a_pattern <- ",\\sa$" # to remove ", a" from end of movie title

nyt_movies <-  nyt_movies_raw |>
  # Unnest `keywords` list variable
  unnest(keywords, keep_empty = TRUE) |>
  # Correct date column format
  mutate(
    pub_date = as_datetime(pub_date),
    media = str_match(value, media_pattern)[, 2] |>
      tolower(),
    # Clean movie titles, removing "the"
    name = str_match(value, name_pattern)[, 2] |>
      tolower() |>
      str_replace(comma_the_pattern, "") |>
      str_replace(the_pattern, "") |>
      str_replace(a_pattern, "") |>
      str_replace(comma_a_pattern, "")
  ) |>
  # Filter data to movies only (removing keyword rows for crew, other types of media, etc.)
  filter(media == "movie") |>
  # Reorder reviews by publication date
  arrange(desc(pub_date)) |>
  # Select variables of interest: columns w/ text for sentiment analysis and merging
  select(
    abstract,
    lead_paragraph,
    name,
    pub_date,
    headline_main,
    headline_kicker,
    headline_print_headline
  )

head(nyt_movies)
```

Above is our merged data set.

## Load Letterboxd data and clean

Load Letterboxd data and clean variables. We perform the same cleaning transformations to the movie `name` variable that we did with the variable of the same name in the NYT data above. Also, we drop movies released before the earliest review in the NYT data (`r min(nyt_movies$pub_date)`); we don't want to merge these older Letterboxd movies since it's unlikely that an NYT critic would review a movie more than a year after its release.

```{r}
#| label: load letterboxd
  
# load letterboxd data
letterboxd_movies <-
  read_csv(
    "https://raw.githubusercontent.com/naomibuell/DATA607_FinalProject/main/movies_trimmed.csv"
  ) |>
  drop_na(minute) |>
  mutate(
    name = tolower(name) |> # switching movie names to lower case
      # Use str_match() with the pattern
      str_replace(comma_the_pattern, "") |>
      str_replace(the_pattern, "") |>
      str_replace(a_pattern, "") |>
      str_replace(comma_a_pattern, "")
  ) |>
  select(-c(id)) |> 
  # only keep Letterboxd data within a similar date range to the NYT data
  filter(date >= year(min(nyt_movies$pub_date)) - 1)

head(letterboxd_movies)
```

## Merge NYT and Letterboxd Data

Below, we merge this NYT data with Letterboxd data based on movie `name`. We also pick the best matches based on when the NYT critic reviewed the movie and when Letterboxd says the movie was released, assuming a true match would show the movie reviewed by NYT right when it came out (since Letterboxd only posts the year of release and not the full date, we assume all movies came out on Christmas day for the purposes of this calculation).

```{r}
#| label: merge Letterboxd and NYT
#| warning: false
#| message: false

# First, merge datasets by name. This is a many to many join, so movie-review rows will not be unique, with some extra matches done in error.
merged <- inner_join(nyt_movies, letterboxd_movies) |>
  
  # For movies w/ multiple matched rows, choose absolute difference in dates between sources.
  group_by(name) |>
  mutate(
    # assuming all movies came out on Christmas (popular date for movie release),
    release_date = as_datetime(paste0(date, "-12-25")),
    # calculate absolute diff btwn review and release date
    dates_diff = abs(difftime(pub_date, release_date, units = "days"))
  ) |> 
  filter(dates_diff == min(dates_diff)) |> # for each unique name, get movie row w/ shortest diff between review and release date
  ungroup() |>
  # remove any duplicated rows
  distinct()

head(merged)
```

## Describe and Validate Analysis Data set

Now that we have our analysis data set, we can perform a few statistical analyses to describe and validate the merged data. The following section checks our data set that we'll use for analysis for outliers, consistency, completeness, and any discrepancies between the merged data sets that would indicate merge errors.

### Descriptive statistics

First, we generate summary statistics and compare the distribution of the dates reported by both data sets:

```{r}
#| label: numeric descriptive stats - dates
#| message: false
#| warning: false

merged  |> 
  select(pub_date, date) |> 
  summary()

# compare dates for NYT and Letterboxd data
merged |>
  ggplot() +
  geom_histogram(
    aes(x = pub_date, fill = "Merged Review publication date (source: NYT)"),
    alpha = .5,
    bins =  44
  ) +
  geom_histogram(
    aes(x = release_date, fill = "Merged Movie release year (source: Letterbox)"),
    alpha = .5,
    bins = 44
  ) +
  theme(legend.position = "bottom") +
  labs(title = "Dates, by data source",
       x = "Date",
       y = "Frequency")
```

Both movie release and review publication dates align well between data sources. They start and end around the same time. Both have peaks in the early 2000's and around 2020's, with a major dip in the late 2000's. This lack of data around the late 2000's stems from the limited NYT data, not the Letterboxd data.

Next we'll check the distribution and outliers of the other numerical data, all from the Letterboxd data set:

```{r}
#| label: Check movie duration (mins) for outliers

summary(merged$minute)

merged |>
  ggplot(aes(x = minute/60)) +
  geom_histogram(bins = 60) +
  labs(title = "Movie length",
       x = "Length in hours (source: Letterboxd)")
```

The distribution is centered around about `r round(median(merged$minute/60), 2)` hours. There are many high outliers in terms of movie length (max `r round(max(merged$minute/60))` hours)–likely due to full television or movie series being included in the database. We elect not to remove these outliers because they can be genuine matches. For e.g., the longest film in the data, "Heimat" was 924 minutes long according to Letterboxd and was reviewed by the NYT, confirming that this chronicle of Germany "will be broadcast for the first time on cable on Bravo in eight parts."

Next we check user ratings for outliers:

```{r}
#| label: summarize rating
#| message: false
#| warning: false

summary(merged$rating)

merged |>
  ggplot(aes(x = rating)) +
  geom_histogram() +
  labs(title = "User ratings",
       x = "Average movie user rating (source: Letterboxd)")
```

Average movie ratings in this data-set range from `r min(merged$rating)` to `r max(merged$rating)`. The average average `rating` on Letterboxd is `r mean(movies$rating)`. The data looks normal but is slightly left skewed. This is expected.

```{r}
#| label: Check ratings for outliers

merged |>
  ggplot(aes(x = rating)) +
  geom_boxplot() 
```

### **Data Quality Assessment/Overlap Analysis**

Here, we check the number of records and variables from each source that were merged. The Letterboxd data was much larger than what we pulled from NYT using the API (`r nrow(letterboxd_movies)` vs. `r nrow(nyt_movies_raw)` movies long, respectively), so the NYT data was the limiting factor for the merge in terms of the length of our final data set.

```{r}
#| label: percent merged

perc_merged <- nrow(merged)/nrow(nyt_movies)
```

However, `r round(perc_merged)*100` percent of data from NYT was able to be merged with a Letterboxd rating.

Here, we identify any duplicate records:

```{r}
#| label: check duplicate movie rows

duplicates <- merged |>
  group_by(name) |>
  mutate(duplicate = n() > 1) |>
  filter(duplicate)

head(duplicates)

num_duplicated_rows <- duplicates |>
  nrow()/2
```

There are `r duplicate_rows` duplicated movies, where one movie `name` from the NYT matched to two observations with the same movie `name` (and release year) in the Letterboxd database. We can manually review to determine which observation is a true match, then drop the other observation(s).

```{r}
#| label: manually remove duplicate rows?



```

## Sentiment and Regression Analysis of NYT Review

Insert sentiment analysis here:

```{r}
#| label: sentiment analysis


```

Create graphic:

```{r}
#| label: conclusion graphic
```

## Conclusion

\[Add conclusion text here\]
